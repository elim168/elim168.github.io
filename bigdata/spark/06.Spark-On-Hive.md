# Spark On Hive

Spark可以从Hive读取数据进行计算，也写入数据到Hive中。

1. 在hive中需要以`hive --service metastore`启动
2. 在spark的conf目录下新增hive-site.xml文件，内容如下：

```xml
<configuration>
        <property>
                <name>hive.metastore.uris</name>
                <value>thrift://hadoop-master:9083</value>
        </property>

</configuration>

```

> 上面的hadoop-master是启动hive的机器的IP或hostname。


以下是一段示例的Spark访问Hive的代码。关键是SparkSession中的`enableHiveSupport()`。

```scala

package com.elim.study.spark.sql

import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

object SparkOnHive {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("SparkOnHiveTest")
    val session = SparkSession.builder().appName("SparkOnHiveTest").enableHiveSupport().getOrCreate()
    import session.sql
    // 指定数据库
    sql("USE TEST")
    // 从Hive的表中查询数据
    val dataFrame = sql("SELECT * FROM PERSON")
    dataFrame.show()
    dataFrame.printSchema()
    // 写入数据到Hive的表中
    dataFrame.write.mode(SaveMode.Overwrite).saveAsTable("spark_hive_person")
    session.stop()
  }

}

```

把上面的程序打包后，像正常的Spark任务那样提交即可。

```shell
bin/spark-submit --master spark://hbase-node1:7077 --class com.elim.study.spark.sql.SparkOnHive /root/spark-sql-1.0.jar
```
