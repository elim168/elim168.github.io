# hive的安装

hive是建立在hadoop的基础上进行工作的，所以安装hive前需要安装了hadoop。hive的官网是[http://hive.apache.org/](http://hive.apache.org/)，我从官网下载了apache-hive-3.1.2-bin.tar.gz。运行`tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /opt`把它解压到`/opt`目录下，运行`mv /opt/apache-hive-3.1.2-bin /opt/hive-3.1.2`把目录重命名为hive-3.1.2。

## 添加环境变量

在用户home目录下的.bashrc文件的末尾增加如下内容。

```shell
export HIVE_HOME=/opt/hive-3.1.2
export PATH=$HIVE_HOME/bin:$PATH
```

然后运行`source ~/.bashrc`使其生效。


## 配置

在`/opt/hive-3.1.2/conf`目录下新增hive-site.xml文件（可配置的参数也可以参考conf目录下的`hive-default.xml.template`文件），其内容如下：

```xml
<configuration>
        <!-- 指定数据存放在hdfs的目录 -->
        <property>
        		<name>hive.metastore.warehouse.dir</name>
        		<value>/user/elim/hive_remote/warehouse</value>
        </property>
        <!--指定存放元数据的数据库的URL地址，下面连接的数据库的主机名是mysql5.7，数据库名是hive_remote。指定了URL后元数据就是远程模式，存在远程数据库中，否则就是本地模式。-->
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://mysql5.7/hive_remote?createDatabaseIfNotExist=true&amp;useSSL=false</value>
        </property>
        <!--数据库驱动名称-->
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>123456</value>
        </property>
</configuration>
```

上面指定使用的数据库是MYSQL，我们需要把MYSQL的驱动包放到`/opt/hive-3.1.2/lib`目录下。然后需要运行`schematool -dbType mysql -initSchema`初始化数据库相关信息到我们指定的MYSQL数据库。


然后基本上就可以启动hive了，可以在命令行执行`hive`命令启动hive客户端。启动时会报错，因为hive里面使用的guava的版本比hadoop中的低，需要拷贝hadoop安装目录下的`share/hadoop/common/lib`下的guava包到hive的lib目录下，然后删除hive自身比较老的包。

## 创建hive数据库

运行`hive`命令启动hive后可以通过`create database $databaseName`命令创建hive数据库。如下命令一开始通过`show databases`看到默认只有一个default数据库，通过`create database test`创建了一个名为test的数据库，之后再查看数据库可以看到有default和test两个数据库。

```text
hive> show databases;
OK
default
Time taken: 1.111 seconds, Fetched: 1 row(s)
hive> create database test;
OK
Time taken: 0.205 seconds
hive> show databases;
OK
default
test
Time taken: 0.076 seconds, Fetched: 2 row(s)
hive> 
```

数据库建立后如果需要切换到刚建立的test数据库时需要使用`use test;`。

> 相应的DDL操作可以参考[这里](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL?src=contextnavchildmode)。
> DML操作可以参考[这里]()。

## 创建表

建表的语法如下：

```sql
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)
  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]
  [COMMENT table_comment]
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]
     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
     [STORED AS DIRECTORIES]
  [
   [ROW FORMAT row_format] 
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)
  ]
  [LOCATION hdfs_path]
  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)
  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)
```

下面的语句创建一个名为person的表，其中字段LIKES是数组类型，字段ADDRESS是map类型。ROW FORMAT后的内容指定了每行的格式定义。

```sql
CREATE TABLE person4
(
    ID INT,
    NAME STRING,
    LIKES ARRAY<STRING>,
    ADDRESS MAP<STRING, STRING>
)
ROW FORMAT 
DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '-'
        MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\n'
```

## 导入数据

按照person表的格式我们在本地定义了一个文件person.data，其内容如下：

```text
1,用户user1,A-B-C,ABC:123-BBB:444
2,user2,A-D-C,ABC:125-BBB:445
3,user3,A-E-C,ABC:128-BBB:446
4,user4,A-F-E,ABC:129-BBB:455
5,user5,A-G-E,ABC:122-BBB:456
6,user6,A-F-H,ABC:123-BBB:470
7,user7,A-E-D,ABC:125-BBB:472
8,user8,A-D-E,ABC:129-BBB:480
9,user9,A-H-G,ABC:128-BBB:488
10,user10,G-B-C,ABC:124-BBB:492
```

然后可以通过如下语句把本地的`/root/person.data`文件的内容导入到表person。

```sql
LOAD DATA LOCAL INPATH '/root/person.data' INTO TABLE person;
```

查询出来数据如下：

```text
hive> LOAD DATA LOCAL INPATH '/root/person.data' INTO TABLE person;
Loading data to table test.person
OK
Time taken: 0.221 seconds
hive> select * from person;
OK
1	用户user1	["A","B","C"]	{"ABC":"123","BBB":"444"}
2	user2	["A","D","C"]	{"ABC":"125","BBB":"445"}
3	user3	["A","E","C"]	{"ABC":"128","BBB":"446"}
4	user4	["A","F","E"]	{"ABC":"129","BBB":"455"}
5	user5	["A","G","E"]	{"ABC":"122","BBB":"456"}
6	user6	["A","F","H"]	{"ABC":"123","BBB":"470"}
7	user7	["A","E","D"]	{"ABC":"125","BBB":"472"}
8	user8	["A","D","E"]	{"ABC":"129","BBB":"480"}
9	user9	["A","H","G"]	{"ABC":"128","BBB":"488"}
10	user10	["G","B","C"]	{"ABC":"124","BBB":"492"}
Time taken: 0.143 seconds, Fetched: 10 row(s)
```

把`person.data`文件的数据导入到表person后，在HDFS上的目录`/user/elim/hive_remote/warehouse/test.db/person`下就有了一个文件`person.data`。

> 目录`/user/elim/hive_remote/warehouse`是我们在hdfs-site.xml文件通过`hive.metastore.warehouse.dir`配置的，`test.db`是我们使用的hive数据库名是test，`person`是我们的表名叫person，所以完整目录就是`/user/elim/hive_remote/warehouse/test.db/person`。
> 通过运行`desc FORMATTED person;`查看输出的`Location:           	hdfs://hadoop-master:9820/user/elim/hive_remote/warehouse/test.db/person`也可以看到我们的表person对应的HDFS的存储位置。

## 创建外部表

在创建表时通过加上关键字`EXTERNAL`可以创建一个外部表。

```sql
CREATE EXTERNAL TABLE person
(
    ID INT,
    NAME STRING,
    LIKES ARRAY<STRING>,
    ADDRESS MAP<STRING, STRING>
)
ROW FORMAT 
DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '-'
        MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\n'
```

不加EXTERNAL的表是内部表，它们的区别是在删除表时外部表的数据还会被保留，而内部表不会。


## 删除/修改数据

hive表默认是不支持删除/修改数据这些事务操作的。需要在建表时指定`STORED AS ORC TBLPROPERTIES ('transactional'='true')`。

```sql
CREATE TABLE PERSON
(
    ID INT,
    NAME STRING,
    LIKES ARRAY<STRING>,
    ADDRESS MAP<STRING, STRING>
)
ROW FORMAT 
DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '-'
        MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\n'
STORED AS ORC TBLPROPERTIES ('transactional'='true');
```

在配置中需要增加如下配置：

```xml
<property>
        <name>hive.support.concurrency</name>
        <value>true</value>
</property>
<property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
</property>
<property>
        <name>hive.txn.manager</name>
        <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
</property>
<property>
        <name>hive.compactor.initiator.on</name>
        <value>true</value>
</property>
<property>
        <name>hive.compactor.worker.threads</name>
        <value>4</value>
</property>
```

之后就可以使用INSERT/DELETE/UPDATE语句了。它不能直接在通过LOAD DATA导入文本文件的内容到表了，因为存储的格式变了，此时可以新建临时表，然后通过`INSERT INTO SELECT`语句来插入数据了。如：

```sql
INSERT INTO PERSON SELECT * FROM PERSON2;
```


（注：基于hive3.1.2）
