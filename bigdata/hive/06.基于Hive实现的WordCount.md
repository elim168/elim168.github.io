# 基于Hive实现的WordCount

假设有如下这样一个文本，需要对其中的单词进行统计。

```text
hello world hadoop hive java hbase mapper reduce
hello elim china hadoop hive
java hbase
mapper reduce
hello china
ok
complete
```

先新建如下这样一张表，它只有一个字段`line`，对应文本中的一行。

```sql
hive> CREATE TABLE wordcount_data (
    >     line string
    > );
OK
Time taken: 0.764 seconds
```

把数据导入到刚刚新建的表中。上面的文本文件在我的`/root/wordcount.txt`。

```shell
hive> LOAD DATA LOCAL INPATH '/root/wordcount.txt' INTO TABLE WORDCOUNT_DATA;
Loading data to table test.wordcount_data
OK
Time taken: 0.911 seconds
```

查询一下，可以看到可以正常的查出里面的数据。

```text
hive> select * from wordcount_data;
OK
hello world hadoop hive java hbase mapper reduce
hello elim china hadoop hive
java hbase
mapper reduce
hello china
ok
complete
Time taken: 1.279 seconds, Fetched: 7 row(s)
```

现在单词统计可以使用split函数把每一行的单词拆开。

```text
hive> select split(line,' ') from wordcount_data;
OK
["hello","world","hadoop","hive","java","hbase","mapper","reduce"]
["hello","elim","china","hadoop","hive"]
["java","hbase"]
["mapper","reduce"]
["hello","china"]
["ok"]
["complete"]
Time taken: 0.406 seconds, Fetched: 7 row(s)
```

再通过explode函数把每一行的单词都展开，变成每一个单词一行。

```text
hive> select explode(split(line,' ')) from wordcount_data;
OK
hello
world
hadoop
hive
java
hbase
mapper
reduce
hello
elim
china
hadoop
hive
java
hbase
mapper
reduce
hello
china
ok
complete
Time taken: 0.185 seconds, Fetched: 21 row(s)
```

这个时候每个单词变成一行了，就可以通过group分组统计了。在统计前先创建一个用于保存结果的表。

```text
hive> create table wordcount_result(
    > word string,
    > count_num int
    > );
OK
Time taken: 0.139 seconds
```

之后可以运行下面的语句进行单词统计，并将结果插入结果表中。

```text
hive> FROM (select explode(split(line,' ')) word from wordcount_data) t
    > INSERT INTO wordcount_result
    > SELECT t.word,count(1) GROUP BY t.word ORDER BY count(1) desc;
```

这个时候会发生mapper reduce，在运行完成后可以通过wordcount_data查看单词统计结果。

```text
hive> select * from wordcount_result;
OK
hello	3
reduce	2
mapper	2
java	2
hive	2
hbase	2
hadoop	2
china	2
world	1
ok	1
elim	1
complete	1
Time taken: 0.178 seconds, Fetched: 12 row(s)
```








