# 动态分区

之前我们通过创建了分区表，通过`LOAD DATA`导入数据到分区表时必须指定一个分区，这种叫静态分区。本文将介绍<font color="red">两种</font>基于数据动态分区的方法。假设现在有下面这样一些数据，需要把它们导入到分区表时按照最后一个字段age进行分区。

```text
1,用户user1,A-B-C,ABC:123-BBB:444,30
2,user2,A-D-C,ABC:125-BBB:445,30
3,user3,A-E-C,ABC:128-BBB:446,35
4,user4,A-F-E,ABC:129-BBB:455,30
5,user5,A-G-E,ABC:122-BBB:456,35
6,user6,A-F-H,ABC:123-BBB:470,35
7,user7,A-E-D,ABC:125-BBB:472,30
8,user8,A-D-E,ABC:129-BBB:480,30
9,user9,A-H-G,ABC:128-BBB:488,40
10,user10,G-B-C,ABC:124-BBB:492,40
```

创建如下这样一个分区表：

```sql
CREATE TABLE PERSON1
(
    ID INT,
    NAME STRING,
    LIKES ARRAY<STRING>,
    ADDRESS MAP<STRING, STRING>
)
PARTITIONED BY (AGE INT)
ROW FORMAT 
DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '-'
        MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\n'
```

> 分区字段对应文本数据的最后一个字段

使用下面的语句把数据文件中的数据导入到表PERSON1中。

```text
hive> LOAD DATA LOCAL INPATH '/root/person_age.data' INTO TABLE PERSON1;
Query ID = root_20200707150851_1085afeb-2a01-48b4-9fe7-c2fabed26465
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1594133290099_0003, Tracking URL = http://hadoop-master:8088/proxy/application_1594133290099_0003/
Kill Command = /opt/hadoop-3.2.1/bin/mapred job  -kill job_1594133290099_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2020-07-07 15:08:59,375 Stage-1 map = 0%,  reduce = 0%
2020-07-07 15:09:04,621 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.96 sec
MapReduce Total cumulative CPU time: 1 seconds 960 msec
Ended Job = job_1594133290099_0003
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://hadoop-master:9820/user/elim/hive_remote/warehouse/test.db/person1/.hive-staging_hive_2020-07-07_15-08-51_882_7595733061559940782-1/-ext-10000
Loading data to table test.person1 partition (age=null)


	 Time taken to load dynamic partitions: 0.193 seconds
	 Time taken for adding to write entity : 0.0 seconds
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.96 sec   HDFS Read: 6209 HDFS Write: 460 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 960 msec
OK
person1__temp_table_for_load_data__.id	person1__temp_table_for_load_data__.nameperson1__temp_table_for_load_data__.likes	person1__temp_table_for_load_data__.address	person1__temp_table_for_load_data__.age
Time taken: 14.171 seconds
```

> 可以看到使用`LOAD DATA`导入数据到分区表时hive内部进行了mapreduce计算，而我们之前在导入数据到非分区表时文件是直接上传到了hdfs，没有mapreduce过程，速度非常快。

查询表PERSON1的结果如下：

```text
hive> SELECT * FROM PERSON1;
OK
person1.id	person1.name	person1.likes	person1.address	person1.age
1	用户user1	["A","B","C"]	{"ABC":"123","BBB":"444"}	30
2	user2	["A","D","C"]	{"ABC":"125","BBB":"445"}	30
3	user3	["A","E","C"]	{"ABC":"128","BBB":"446"}	35
4	user4	["A","F","E"]	{"ABC":"129","BBB":"455"}	30
5	user5	["A","G","E"]	{"ABC":"122","BBB":"456"}	35
6	user6	["A","F","H"]	{"ABC":"123","BBB":"470"}	35
7	user7	["A","E","D"]	{"ABC":"125","BBB":"472"}	30
8	user8	["A","D","E"]	{"ABC":"129","BBB":"480"}	30
9	user9	["A","H","G"]	{"ABC":"128","BBB":"488"}	40
10	user10	["G","B","C"]	{"ABC":"124","BBB":"492"}	40
Time taken: 0.151 seconds, Fetched: 10 row(s)
```

> 如果你到hdfs上去看表PERSON1对应的数据文件，也可以看到对应的分区目录。

上面使用`LOAD DATA`导入数据到分区表中没有指定固定分区时，默认会把最后的列作为分区字段，有多个分区字段就是最后几列。如果我们的分区字段在数据文件中对应的不是最后一个字段就会有问题。此时需要我们通过中间表来进行导入。假设我们的数据是如下这样：

```text
1,用户user1,30,A-B-C,ABC:123-BBB:444
2,user2,30,A-D-C,ABC:125-BBB:445
3,user3,35,A-E-C,ABC:128-BBB:446
4,user4,30,A-F-E,ABC:129-BBB:455
5,user5,35,A-G-E,ABC:122-BBB:456
6,user6,35,A-F-H,ABC:123-BBB:470
7,user7,30,A-E-D,ABC:125-BBB:472
8,user8,30,A-D-E,ABC:129-BBB:480
9,user9,40,A-H-G,ABC:128-BBB:488
10,user10,40,G-B-C,ABC:124-BBB:492
```

可以看到上面的数据的AGE是作为第三个字段的，此时我们先创建一个普通的表person2,用于临时存储上面的数据。

```sql
CREATE TABLE PERSON2
(
    ID INT,
    NAME STRING,
    AGE INT,
    LIKES ARRAY<STRING>,
    ADDRESS MAP<STRING, STRING>
)
ROW FORMAT 
DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '-'
        MAP KEYS TERMINATED BY ':' LINES TERMINATED BY '\n'
```

使用下面的语句把数据文件中的数据导入到表PERSON1中。

```text
hive> LOAD DATA LOCAL INPATH '/root/person2_age.data' INTO TABLE PERSON2;
Loading data to table test.person2
OK
Time taken: 0.185 seconds
```

查询表PERSON2的结果如下：

```text
hive> SELECT * FROM PERSON2;
OK
person2.id	person2.name	person2.age	person2.likes	person2.address
1	用户user1	30	["A","B","C"]	{"ABC":"123","BBB":"444"}
2	user2	30	["A","D","C"]	{"ABC":"125","BBB":"445"}
3	user3	35	["A","E","C"]	{"ABC":"128","BBB":"446"}
4	user4	30	["A","F","E"]	{"ABC":"129","BBB":"455"}
5	user5	35	["A","G","E"]	{"ABC":"122","BBB":"456"}
6	user6	35	["A","F","H"]	{"ABC":"123","BBB":"470"}
7	user7	30	["A","E","D"]	{"ABC":"125","BBB":"472"}
8	user8	30	["A","D","E"]	{"ABC":"129","BBB":"480"}
9	user9	40	["A","H","G"]	{"ABC":"128","BBB":"488"}
10	user10	40	["G","B","C"]	{"ABC":"124","BBB":"492"}
Time taken: 0.142 seconds, Fetched: 10 row(s)
```

此时我们可以通过如下SQL通过把从PERSON2查询出来的字段按照PERSON1中的列的顺序查询出来插入到分区表PERSON1中。

```sql
FROM PERSON2
INSERT INTO PERSON1
SELECT id,name,likes,address,age
```

运行上述语句后hive内部也会进行mapreduce操作，这样也达到了动态分区的目的。

```text
hive> FROM PERSON2
    > INSERT INTO PERSON1
    > SELECT id,name,likes,address,age;
Query ID = root_20200707152456_9a86d88e-2c67-4579-9701-f95aa2f78bb6
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1594133290099_0004, Tracking URL = http://hadoop-master:8088/proxy/application_1594133290099_0004/
Kill Command = /opt/hadoop-3.2.1/bin/mapred job  -kill job_1594133290099_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2020-07-07 15:25:03,214 Stage-1 map = 0%,  reduce = 0%
2020-07-07 15:25:08,474 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.45 sec
MapReduce Total cumulative CPU time: 2 seconds 450 msec
Ended Job = job_1594133290099_0004
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://hadoop-master:9820/user/elim/hive_remote/warehouse/test.db/person1/.hive-staging_hive_2020-07-07_15-24-56_081_2569549228893417320-1/-ext-10000
Loading data to table test.person1 partition (age=null)


	 Time taken to load dynamic partitions: 0.192 seconds
	 Time taken for adding to write entity : 0.0 seconds
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 2.45 sec   HDFS Read: 6758 HDFS Write: 460 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 450 msec
OK
id	name	likes	address	age
Time taken: 13.821 seconds
```

> 本文介绍的是基于一个分区字段的操作，如果有多个分区字段，也是同样的操作。

（注：本文使用的hive3.1.2）

## 参考文档

* [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-InsertingdataintoHiveTablesfromqueries](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-InsertingdataintoHiveTablesfromqueries)




